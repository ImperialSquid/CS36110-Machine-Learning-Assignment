\section{Initial Building of Machine Learning Models}
\subsection{Initial Classifier Creation and Explanation}
For this assignment I used a J48 and NaiveBayes classifier as prescribed in the assignment, however I also used a RandomForest classifier as I had a passing knowledge of it from previous machine learning work. 

J48 is an implementation of a decision tree-based classifier. It works by arranging the features into a tree where at each level the feature chosen splits the dataset with the most homogeneity in each branch. One advantage of J48 is that it is a very simple model to both produce and, arguably more importantly in some situations, a explain to someone who has little or no knowledge of machine learning.

NaiveBayes is a probabilistic classifier, meaning it uses the given dataset to find the probability of an instance belonging to a particular decision group. This is done by using each feature to update the likelihood that an instance belongs to a given class. NaiveBayes has the advantage of disregarding the interactions between features (hence why it's naive) this reduces the calculations needed greatly. It also easily adapts to more data as there is nothing to restructure such as a decision tree.

The last classifier I used is a RandomForest classifier as I had encountered it before. A RandomForest is an example of an ensemble learning model, meaning that it is a model made up of smaller models, attempting to negate its/their negatives. A RandomForest is made up of lots of decision trees, the difference being, each is trained on a subset of the data and at each level it picks from a subset of the features, then, when classifying, a majority vote is taken between them. This greatly reduces the over-fitting decision trees are liable to have happen \cite{liaw2002}.

\subsection{Comparison Between Classifiers}
When comparing machine learning models there are many ways to measure and compare them. We will be looking at the F1 scores as well as the ROC area.

A useful way to compare classifiers is to look at their F1 scores, this is the harmonic mean between the precision (the number of true positives divided by all predicted positives, how often the model is correct when it thinks it is) and the recall (the number of true positives divided by actual positives, how many positives the model misses) \cite{sasaki2007}. Due to the nature of the precision and recall there is often a trade-off between them, therefore by considering both we can very effectively evaluate a model's accuracy. 

Another useful metric is the ROC area, this compares the model to a random guess by graphing the True Positive Rate (aka the recall as mentioned before) and False Positive Rate (FPR is the false positive prediction divided by all the actual negative predictions, it measures the positives given where a negative was correct) against each other at various levels of threshold, the ROC area being the area beneath the curve as plotted, 0.5 indicating a classifier being no better than a random guess and 1 being perfect classification. 

Below are the score for each classifier, the best in bold, as we can see RandomForest outperformed both NaiveBayes and J48 in both metrics.

\begin{center}
\begin{tabular}{|r|l|l|}
\hline 
Classifier & F1 Measure & ROC Score \\ 
\hline 
NaiveBayes & 0.549 & 0.782 \\ 
\hline 
J48 & 0.718 & 0.854 \\ 
\hline 
RandomForest & \textbf{0.788} & \textbf{0.934} \\ 
\hline 
\end{tabular} 
\end{center}


\subsection{A Baseline for Comparison}
A very basic baseline would be a "blanket" assignment of a particular decision class, under this approach we would give a result of, for example, "A" to any data given, in this case resulting in approximately a 25\% accuracy rate, given all the classifiers achieved higher than this we can say they are all at least better than this "blanket" approach, this is a fairly unsurprising result as a blanket baseline is particularly useful in unbalanced datasets however this is not (being an approximately equal split between each 4 decision classes).

Another good baseline would be a random approach, such as that used when comparing ROC scores, given that a 0.5 ROC score is equivalent with a model being no better than a "coin flip" approach and all of the three models discussed beat this by a considerable margin we can say they surpass this baseline too. 

A third baseline and a well-accepted industry standard for comparison is the NaiveBayes classifier in itself, of course NaiveBayes is comparable with itself so that classifier is good enough and referring back to the comparison made in section 1.2 we can say that RandomForest and J48 are both good enough too given they did better in both F1 measure and ROC score than NaiveBayes.

