\section{Decision Trees Comparison}

As per the assignment I used WEKA to remove the \textit{Month\_of\_absence}, \textit{Day\_of\_the\_week}, \textit{Season}, \textit{CommuteDistance}, \textit{YearsService}, \textit{WorkLoad}, \textit{Education} and  \textit{Smoker} features before training a new J48 classifier to see how this would affect it's performance. As before we will compare their F1 and ROC metrics to evaluate performance at a glance. 

\begin{center}
\begin{tabular}{|r|l|l|}
\hline 
J48 & F1 Measure & ROC Score \\ 
\hline 
Original & \textbf{0.718} & 0.854 \\ 
\hline 
Reduced & 0.700 & \textbf{0.862} \\ 
\hline 
\end{tabular} 
\end{center}

As we can see despite the huge reduction in data given the classifier was still trained to a relatively good degree, in fact neither the original or reduced dataset models won outright.

This lack of a big difference between them implies that the data excluded was unnecessary for a good quality of classification.

Next I looked into the actual structure of the trees. The first observation I made was that the original tree would often split the data  on \textit{Month\_of\_absence} and \textit{Day\_of\_the\_week} making for a very wide tree with a lot of leaves, according to WEKA the original had 157 leaves but the reduced only had 87. While the original often split the data with \textit{Month\_of\_absence} or \textit{Day\_of\_the\_week} under the reduced dataset it makes frequent use of \textit{Target}.

My next observation was that the reduced dataset tree was not only less wide but also less deep, it only had seven levels  at it's deepest point and normally only went 3 or 4 deep, whereas the original went 9 deep twice and usually was about 5 levels deep. This means that the reduced tree was much more efficient with its classifying.

I think this is a different classification problem as the original because the trees were built from different datasets in that they had a different selection of features, the results were massively different because J48 is a greedy algorithm and it makes some sense that the features that split the dataset best would be those with the most values, however this made the original actually very inefficient. The difference in problem is plain to see in that the trees had very different depths, leaf counts and parameters at each level after the first.

\textit{Final word count: 1726}
