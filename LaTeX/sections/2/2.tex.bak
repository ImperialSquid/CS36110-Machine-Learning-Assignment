\section{Dataset Imputation and Reclassification}
\subsection{Dataset Imputation and Justification}
The dataset given with this assignment has several missing values in it, namely 46 (7\%) in \textit{Reason\_for\_absence} and 3 (\textless 0.5\%) in \textit{Month\_of\_absence}. There are a number of ways to classify missing values, namely:
\begin{itemize}
\item MCAR or Missing Completely At Random (there is no relationship between the missingness and the data)
\item MAR or Missing at Random (there is a relationship between the missingness and the data but not the missing value)
\item MNAR or Missing Not At Random (the missing data and the rest have a strong relationship)
\end{itemize}

I imported the dataset into R \cite{langR} so I could analyse the data (using the package farff \cite{packfarff}). If the \textit{Reason\_of\_absence} was MCAR the distribution of each feature for both missing and non-missing reasons would be the same, however we see this is not the case (plots were made with ggplot2 \cite{packggplot2}). As can be observed there is a vast difference between the distributions of each feature depending on if there's an associated reason for the absence. 

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/plot_missing_edu.png}
  \label{fig:plot_missing_edu}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/plot_missing_smoker.png}
  \label{fig:plot_missing_smoker}
\end{minipage}
\caption{Some plots to show the percentage missing depends on a feature's value and is not uniform}
\label{fig:missingpercent}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/plot_dist_weight.png}
  \label{fig:plot_dist_weight}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/plot_dist_perf.png}
  \label{fig:plot_dist_perf}
\end{minipage}
\caption{Some plots to show the distribution of a feature depends on the missingness of the reason and is not identical}
\label{fig:missingdist}
\end{figure}

The fact that the missing data is not MCAR, it represents a significant proportion of our dataset and removing it would change the distributions of our features, we must therefore impute the data \cite{cheema2014}. Given the data was already R, I used a package called mice (Multivariate Imputation by Chained Equations) \cite{packmice}. mice works by replacing each variable with a starting replacement (usually the mean or mode) it then using all of the other variables to impute each column one by one, replacing those values that were originally missing, the idea being that after many rounds of this the values will converge together \cite{azur2011}. 

There were several reasons for using a MICE approach, namely, it works with MAR data as we proved was the case, it respects the existing variance of the features unlike a mean or mode replacement (such as what WEKA's filters use) and it works with categorical data unlike most imputation methods. However it should be acknowledged that MICE has flaws, for example, research has yet to be done on the best number of iterations to perform and there is no mathematical basis to this method of imputation unlike other methods.

\subsection{Reclassification and Re-Evaluation}
After imputation next was to reclassify on the new data and compare against the old classifiers. I loaded the .arff file into WEKA and retrained the same three classifiers, NaiveBayes, J48 and RandomForest. Again we will compare their F1 measures and ROC scores, the better of the two being marked in bold.

\begin{center}
\begin{tabular}{|r|l|l|l|l|}
\hline 
Classifier & \multicolumn{2}{l|}{F1 Measure} & \multicolumn{2}{l|}{ROC Score} \\ 
\hline 
 & Old & New & Old & New \\ 
\hline 
NaiveBayes & 0.549 & \textbf{0.554} & 0.782 & \textbf{0.784} \\ 
\hline 
J48 & \textbf{0.718} & 0.698 & \textbf{0.854} & 0.847 \\ 
\hline 
RandomForest & \textbf{0.788} & 0.751 & \textbf{0.934} & 0.858 \\
\hline 
\end{tabular}
\end{center}

Interestingly, and personally unexpectedly, we can see that NaiveBayes did slightly better (though really negligibly so) whereas J48 did slightly worse and RandomForest even more so. Most likely this is due to faults in the imputation method, maybe some features should have been excluded from contributing towards the imputation and the method of making the models could have been wrong too, here mice used multinomial logistic regression but maybe another method would be more appropriate. Of course the possibility can't be ignored that this method of imputation was inappropriate too, maybe training a classifier on complete case data and using it to find the missing values would have worked better. 